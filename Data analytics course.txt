#Creating a list and printing its items
x = [1, 2, "a", "b"]

for i in x:
    print(i)

#OR
i = 0
while i = len(x):
      print(x[i])
      i = i+1

#Slicing operation
x = "This is a string"
print (x[0:3])                                   #returns 'Thi'

x = 'Dr. Christopher Brooks'
print(x[4:15])                                   #returns 'Christopher', remember 'space' also has an index value

#Other operations with string
first_name = "Christopher"
last_name = "Brooks"
print(first_name + " " + last_name)              #Concatenation(+), returns 'Christopher Brooks'
print(first_name * 2)                            #repeatation(*), returns 'ChristopherChristopher'
print("Chris" in first_name)                     #search(in), returns 'True'

first_name = "Christopher Arthur Brooks".split(" ")[0]         #split based on space
last_name = "Christopher Arthur Brooks".split(" ")[-1]

#Dictionary
x = {1: "avra", 2: "christopher"}

for i in x.keys():
    print(x[i])                                  #returns '1, 2'

for i in x.values():
    print(x[i])                                  #returns 'avra, christopher'

for i, j in x.items():
    print(i,j)                                   #returns both key-value pairs

#Defining a variable - to add or substract
def do_math(a, b, kind):
  if (kind=='add'):
    return a+b
  else:
    return a-b
do_math(1, 2, 'add')

#Using Map function
people = ['Dr. Christopher Brooks', 'Dr. Kevyn Collins-Thompson', 'Dr. VG Vinod Vydiswaran', 'Dr. Daniel Romero']
def split_title_and_name (person):
    title = person.split()[0]
    lastname = person.split()[-1]
    return '{} {}'.format(title, lastname)
list(map(split_title_and_name, people))

#Using Lambda function
people = ['Dr. Christopher Brooks', 'Dr. Kevyn Collins-Thompson', 'Dr. VG Vinod Vydiswaran', 'Dr. Daniel Romero']
def split_title_and_name(person):
    return person.split()[0] + ' ' + person.split()[-1]
list(map(split_title_and_name, people)) == list(map(lambda person: person.split()[0] + ' ' + person.split()[-1], people))

#Datetime & Time function
import datetime as dt
import time as tm
tm.time()                                                          

dt_now = dt.datetime.fromtimestamp(tm.time())
dt_now

delta = dt.timedelta(days=100)                                  #100 days before today
today = dt.date.today()
print(today-delta)                                              #will print the date 100 days before

#NUMPY
import numpy as np
a = np.array([1,2,3])
print(a)
print(a.ndim)                                                         #returns '1'
print(a.shape)                                                        #returns (3,1)
print(a.dtype)                                                        #returns 'int32', one row takes 32 byte

b = np.ones((2,3))
b                                                                     #(2,3) matrix with all elements as '1'

c = np.random.rand(2,3)
print(c)

d = np.arange(10,20,2)                                                #output - 'array([10, 12, 14, 16, 18])'
d                                                                     #from 10 to 15 with difference of 2

e = np.linspace(10,15,10)                                             #10 numbers from 10 to 15
e                                                                     #LINSPACE - check spelling

farenheit = np.array([0, 37, 98, 103])
celcius = (farenheit + 32)/(1.8)
celcius

h = np.arange(1,16).reshape(3,5)                                      #elements 1-15, reshaped into (3,5)
h

#MATRIX MULTIPLICATION
f = np.array([[1,2], [3,4]])                                          #two [] brackets has to be present
g = np.array([[4,5], [6,7]])
print(f@g)                                                            #matrix multiplication, not element-wise multiplication

#IMAGE module from PIL(python image library)
from PIL import Image
from IPython.display import display
im = Image.open("C:/Users/Administrator/Downloads/rupsa.jpg")
display(im)                                                           #printing the image
pic_array = np.array(im)
print(pic_array)
print(pic_array.shape)                                                #output - '(800, 640, 3)'

mask = np.full(pic_array.shape, 255)                                  #to make all elements in the array as '255'
mask

modified_array = mask-pic_array                                       #substracting each element from (mask - pic_array)
modified_array = modified_array.astype(np.uint8)                      #this line is mandatory to display the new image
modified_array

display(Image.fromarray(modified_array))                              #image with different colour concentration

reshaped_array = np.reshape(modified_array, (200,150))                #only if black&White image
display(Image.fromarray(reshaped_array))

#Indexing
a = np.array([[1,2], [3,4], [5,6]])
a[1,1]                                                                #returns '4'

np.array([a[0,0], a[1,1], a[2,0]])                                    #(1,4,5)
#OR
np.array(a[[0,1,2], [0,1,0]])                                         #(1,4,5)

#Slicing
a = np.array([1,2,3,4,5])
print(a[:3])                                                          #donot print element(3), output-(1,2,3) 
print(a[2:4])                                                         #output - (3,4)

b = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
print(b[:2])                                                          #all elements from 1st-two rows, index '0 & 1'
print(b[:2, 1:3])                                                     #(2,3,6,7)
print(b[:, 0])                                                        #all rows & 1st column

#Datasets with Numpy
df = np.genfromtxt("C:/Users/Administrator/Downloads/vvv.txt")        #to load datasets from external 'csv OR txt' file
df

#REGEX (manipulating text)
import re                                                             #importing 'REGULAR EXPRESSION'
text = "this is a good day"
if re.search("good", text):
    print("Wonderfull")
else:
    print("Alas")                                                     #output- 'wonderfull'

text = "Amy works diligently. Amy gets good grades so students like Amy is successful"
re.findall("Amy", text)                                                                      #output- 'Amy Amy Amy'
re.split("Amy", text)                                                                        #output- '' works diligently. ', ' gets good grades so students like ', ' is successful''

#Patterns & charecter class
grades = "ACAAABCBCBAA"
re.findall("B", grades)                                               #output- 'B B B'
re.findall("AB", grades)                                              #only 1 'AB' is present in the string 'grades'
re.findall("[A][B-C]", grades)                                        #should start with 'A', after that either 'B' or 'C'
re.findall("[^A]", grades)                                            #['C', 'B', 'C', 'B', 'C', 'B'], shows results which is not 'A' 
re.findall("^[A]", grades)                                            #output- ['A'], begining with 'A'

#Using regex on text files & to understand quantifiers
import re
with open("C:/Users/Administrator/Downloads/ferpa.txt", "r") as f:
    wiki = f.read()                                                   #to read a text file called 'ferpa'
wiki

re.findall("[\w]*\[edit\]", wiki)                                     #find all the headings, as "[edit]" is present beside every heading, since txt from wikipedia
re.findall("[\w ]*\[edit\]", wiki)                                    #the previous one only found the last word before edit, but including space after '\w' the whole heading was found
for headings in re.findall("[\w ]*\[edit\]", wiki):
    print(re.split("[\[]", headings)[0])                              #only the headings were printed this time, not the '[edit]'

#OR same output using 'FINDITER' function
for item in re.finditer("([\w ]*)(\[edit\])", wiki):
    print(item.group(1))

#GROUPS in REGEX "( ) ( )"
import re
with open("C:/Users/Administrator/Downloads/ferpa.txt", "r") as f:
    wiki = f.read()
for item in re.finditer("([\w]*)(\[edit\])", wiki):                  #"findall" functions returns 'tuple' & finditer returns 'list'
    print(item.groups())                                             #if only the 'item' is printed then output will show as 'match object'

for item in re.finditer("([\w]*)(\[edit\])", wiki):
    print(item.group(1))                                             #"group(0)" will show both the output & "[edit]"

for item in re.finditer("(?P<title>[\w]*)(?P<link>\[edit\])", wiki):     #can name the different groups with "?P<>", returns a dictionary with 'keys' being the 'name' & 'values' being the output 
    print(item.groupdict()["title"])                                     #to return only the 'title' and not the link                                        

#LOOK-AHEAD & LOOK-BEHIND (before or after the text that we want to isolate)
#throw-away (?=), to match but not to show in output

for item in re.finditer("(?P<title>[\w]*)(?=\[edit\])", wiki):
    print(item.group(1))

#LOOK AHEAD & BEHIND with another wikipedia file
with open("C:/Users/Administrator/Downloads/buddhist.txt", "r") as f:
        wiki = f.read()
wiki

pattern = """
(?P<Name>.*)
(-\ located\ in\ )
(?P<City>\w*)
(,\ )
(?P<State>\w*)"""                                                       #multi-line pattern

for item in re.finditer(pattern, wiki, re.VERBOSE):                     #'VERBOSE' for multi-line pattern
    print(item.groupdict())

#Assignment-1 using regular expressions(regex)
import re
simple_string = ("Amy is 5 years old, and her sister Mary is 2 years old. Ruth and Peter, their parents, have 3 kids.")
def names (string):
    name = " ".join(re.findall(r"[A-Z]\w+", string))
    print(name)
    print(len(name.split()))
names(simple_string)

import re
with open ("C:/Users/Administrator/Downloads/vvv.txt", "r") as file:
    grades = file.read()
re.findall(re.escape("\n") + "(.*)" + re.escape(": B"), grades) 

import re
with open ("C:/Users/Administrator/Downloads/vvv.txt", "r") as file:
    logdata = file.read()
x = logdata.split("\n")
list_all=[] 
dict_log={} 
for item in x:
    pos_1=item.find(' - ')
    IP=item[:pos_1]
    username=re.findall(re.escape(' - ')+"(.*)"+re.escape(' ['),item)[0]
    date=re.findall(re.escape(' [')+"(.*)"+re.escape('] '),item)[0]
    post_request_type=re.findall(re.escape('"')+"(.*)"+re.escape('"'),item)[0]
    dict_log['host'] = IP
    dict_log['user_name'] = username
    dict_log['time'] = date
    dict_log['request'] = post_request_type
    list_all.append(dict_log.copy())
    dict_log.clear()
list_all

#PANDAS(to work on data-frames)
import pandas as pd
students = ["Alice", "Jack", "Moly"]                                  #using 'SERIES' function
pd.Series(students)                                                   #output- "Alice,Jack,Moly dtype: object", 1-D dataframe

numbers = [1, 2, 3,4]
pd.Series(numbers)

#Location function("iloc-index location", "loc-location")- this function only show the 'values' of the dictionary
import pandas as pd
students ={"Alice": "Physics", 
           "Jack": "Chemistry", 
           "Moly": "English", 
           "Sam": "History"}
df = pd.Series(students)                                              #'pd.DataFrame' function cannot be used since only 1 row is present, i.e. only 1 dictionary
df                                                                    #output- 'Alice      Physics, Jack     Chemistry, Moly       English, Sam        History, dtype: object'
df.iloc[3]                                                            #output- 'History'
df.loc["Sam"]                                                         #output- 'History'

#Simple loop to add numbers present in a series
grades = pd.Series( [90, 70, 80, 60] )
total = 0                                                             #initiate a varible where the sum will be stored
for i in grades:                                                      #'i' will be 90,70,80,60 respectively for each loop
    total = total+i
print(total)                                                          #output- '300'
#OR can be also directly be done using 'NUMPY'
import pandas as pd
import numpy as np
grades = pd.Series( [90, 70, 80, 60] )
total = np.sum(grades)
total

#DataFrame (using PANDAS)
import pandas as pd
r1 = pd.Series({"Name": "Alice", "Sub": "Physics", "Score": 85})
r2 = pd.Series({"Name": "Jack", "Sub": "Chemistry", "Score": 82})
r3 = pd.Series({"Name": "Helen", "Sub": "Biology", "Score": 90})
df = pd.DataFrame([r1, r2, r3], index = ["School 1", "School 2", "School 3"])        
print(df)                                                                     #can be directly created using a list where the values are dictionaries r1,r2,r3

df.iloc[2]                                                            #output- "Name       Helen Sub      Biology Score         90 Name: School 3, dtype: object"
df.loc["School 2"]                                                    #cannot mention 'Jack' insted of 'School 2', else error
df.loc["School 2"]["Name"]                                            #output- 'Jack'
df.drop("School 2")                                                   #Deletes entire row "School 2"
df["Ranking"] = ["2nd", "3rd", "1st" ]                                #ADDING a new 'Column' in the DataFrame

#Loading DataFrame from csv file
import pandas as pd
df = pd.read_csv("C:/Users/Administrator/Downloads/admission_predict.csv")
df.head()
df = pd.read_csv("C:/Users/Administrator/Downloads/admission_predict.csv", index_col = 0)                  #to set first column as index
df1 = df.rename(columns = {"GRE Score": "GRE", "University Rating": "UR"})                                 #renaming column names  

#DataFrame queries
mask = df["Chance of Admit "] > 0.7
mask                                                                  #will show only the chance of admit column, and shows 'True' for values which satisfies condtion & 'False' for others
df.where(mask)                                                        #Boolean mask using 'WHERE' function, shows 'NaN' values for entire row whose value is 'False' in above code
df.where(mask).dropna()                                               #removes the rows with 'NaN' values

#OR 'Boolean mask' can be directly applied using single code
df [df["Chance of Admit "] > 0.7]                                     #removes all rows that does not satisfies the condition

#Multiple condition
(df["Chance of Admit "] > 0.7) & (df["SOP"] > 4)                      #will show 'True' or 'False' for every index/rows 

#Indexing DataFrame
import pandas as pd
df = pd.read_csv("C:/Users/Administrator/Downloads/admission_predict.csv", index_col = 0) 
df.head(2)

#OR using 'set_index()' function
df = pd.read_csv("C:/Users/Administrator/Downloads/admission_predict.csv")
df.set_index("Serial No.").head(3)

#To 'RESET' index
df.reset_index().head(3)

df = df[df["University Rating"] == 5]                                 #applying a Boolean mask

#Multi-indexing
df["University Rating"].unique()                                      #shows different values that are present in the column 'University Rating', o/p- "array([4, 3, 2, 5, 1], dtype=int64)"
df = df.set_index(["University Rating", "Serial No."])                #multi-level indexing is set
df                                

#Taking 'census' DataFrame
import pandas as pd
df = pd.read_csv("C:/Users/Administrator/Downloads/census.csv")
df.head(3)

df["SUMLEV"].value_counts()                                           #to count number of different values present in a column

df = df[df["SUMLEV"] == 50]                                           #removing all rows, whose 'SUMLEV' value is other than 50 (i.e. removing 40)
df.head(5) 

#SELECTING only the 'COLUMNS' that we want to KEEP/SHOW
columns_to_keep = ['STNAME','CTYNAME','BIRTHS2010','BIRTHS2011','BIRTHS2012','BIRTHS2013','BIRTHS2014','BIRTHS2015',
                  'POPESTIMATE2010','POPESTIMATE2011','POPESTIMATE2012','POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015']
df = df[columns_to_keep]
df.head(4)

df = df.set_index( ["STNAME", "CTYNAME"] )
df                                                                    #multi-indexing

df.loc[ [('Alabama', 'Baldwin County'), ('Wyoming', 'Weston County')] ]               #DataFrame querying

#Missing Values
import pandas as pd
df = pd.read_csv("C:/Users/Administrator/Downloads/class_grades.csv")                 #'class_grade' dataFrame is loaded
df.head(6)
df.dropna().head(6)                                                   #removes 'NaN' values

#Filling of 'NaN' values (fillna(), Forward filling & Backward filling)
df.fillna(0).head(6)                                                                  #'NaN' is given a value that is assigned by us
df.fillna(method = "ffill")                                           #Forward filling will assign the previous valid value to the 'NaN' values
df.fillna(method = "bfill")                                           #opposite of 'ffill', BEFORE USING 'ffill & bfill' data has to be sorted based on index, else not work

#Woking with 'log' dataset
import pandas as pd
df = pd.read_csv("C:/Users/Administrator/Downloads/log.csv")
df.head(5)

df.sort_index()                                                       #to 'SORT INDEX'
df.set_index(["user", "time"]).head(6)                                #multi-indexing

#Using 'REPLACE' function with help of 'Regex'
pattern = "[\w]*[.]html$"                                             #to search for the given pattern anywhere in the table, so column name need not be mentioned
df.replace(to_replace= pattern, value= "webpage", regex=True)         #replace the whole word which satisfies the pattern by the word 'webpage'

#Manipulating DataFrame
import pandas as pd
df = pd.read_csv("C:/Users/Administrator/Downloads/presidents.csv")
df.head(5)

df["Copy"] = df["President"]                                          #coping the value of the 'President' column into a new column called 'Copy'
df.head(5)

#Splitting the 'President' column into 'First' & 'Last' name using 'APPLY' function
def split_name (row):                                                 #function name is 'split_name', parameter name is 'row'
    row["First"] = row["President"].split(" ")[0]                     #split column 'President' into 2 parts from 'SPACE' and take the index '0'
    row["Last"] = row["President"].split(" ")[-1]                     #split column 'President' into 2 parts from 'SPACE' and take the index '-1'
    return row
df = df.apply(split_name, axis = 1)                                   #use of apply function on function 'split_name' (column-wise), insted of '1' we can also write 'columns'
df.head(5)

del(df["First"])                                    
del(df["Last"])                                                       #DELETING COLUMNS

#Applying Splitting using 'EXTRACT' FUNCTION
pattern = "(^[\w]*)(?:.* )([\w]*$)"                                   #group function is used(3 groups)
df["President"].str.extract(pattern).head()

#OR if we want to give names to our columns
pattern = "(?P<First>^[\w]*)(?:.* )(?P<Last>[\w]*$)"                  #"?P<name>" is for giving names to the column, rest all are the same 
n = df["President"].str.extract(pattern).head()
n                                                                     #prints only the 'First' & 'Last' column
df["First"] = n["First"]   
df["Last"] = n["Last"]                                                #copying rows 'First' & 'Last' into the main DataFrame 'df'
df.head()

#Format change of 'Date' in 'Born' column using 'to_datetime' function
pattern = "([\w]{3} [\w]{1,2}, [\w]{4})"                              #first bracket () is mandatory after ""
df["Born"] = df["Born"].str.extract(pattern)
df["Born"].head()
df["Born"] = pd.to_datetime(df["Born"])                               #using "pd.to_datetime function"
df["Born"].head(1)                                                    #output- "1732-02-22"

#Merging(horizontally) DataFrame (union=outer join, intersection=inner join)
import pandas as pd
staff = pd.DataFrame( [ {"Name": "Kelly", "Role": "HR"},
                     {"Name": "Sally", "Role": "Course liason"},
                     {"Name": "James", "Role": "Grader"} ] )
staff = staff.set_index("Name")
student = pd.DataFrame( [ {"Name": "James", "Role": "Business"},
                     {"Name": "Mike", "Role": "Law"},
                     {"Name": "Sally", "Role": "Engineering"} ] )
student = student.set_index("Name")
print(staff)
print(student)

pd.merge(staff, student, how = "outer", left_index=True, right_index=True)      #A union B
pd.merge(staff, student, how = "inner", left_index=True, right_index=True)      #A intersection B
pd.merge(staff, student, how = "left", left_index=True, right_index=True)       #A
pd.merge(staff, student, how = "right", left_index=True, right_index=True)      #B

#Merging using 'on' parameter(no need for index)
pd.merge(staff, student, how = "outer", on = "Name")                            #A union B, without setting index

#Multi-indexing using 'on' parameter
import pandas as pd
staff = pd.DataFrame( [ {"First_name": "Kelly", "Last_name": "Green", "Role": "HR"},
                     {"First_name": "Sally", "Last_name": "Blue", "Role": "Course liason"},
                     {"First_name": "James", "Last_name": "Red", "Role": "Grader"} ] )
student = pd.DataFrame( [ {"First_name": "James", "Last_name": "Red", "Role": "Business"},
                     {"First_name": "Mike", "Last_name": "Smith", "Role": "Law"},
                     {"First_name": "Sally","Last_name": "Blue", "Role": "Engineering"} ] )
pd.merge(staff, student, how = "outer", on = ["First_name", "Last_name"])

#Concatenation(vertically) of Dataframe
import pandas as pd
frames = [staff, student]
pd.concat(frames)

#PANDAS IDIOMS
#Using 'apply' function within a defined function
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Administrator/Downloads/census.csv")
def min_max(row):
    data = row[ ['POPESTIMATE2010','POPESTIMATE2011','POPESTIMATE2012',
                'POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015'] ]        #double [ [] ] is required to specify- 1.list & 2.index(columns)
    return pd.Series( {"Min": np.min(data), "Max": np.min(data)} )
df.apply(min_max, axis = 1)                                                     #can also be mentioned 'columns' within "" insted of '1' 

#OR To add min & max column to the original DataFrame
def min_max(row):
    data = row[ ['POPESTIMATE2010','POPESTIMATE2011','POPESTIMATE2012',
                'POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015'] ]
    row["min"] = np.min(data)
    row["max"] = np.max(data)
    return row
df.apply(min_max, axis = 1)

#OR (using lambda function)
data = ['POPESTIMATE2010','POPESTIMATE2011','POPESTIMATE2012',
                'POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015'] 
df.apply(lambda x: np.max(x[data]), axis = 1)

#GROUP-BY (to speed up task)
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Administrator/Downloads/census.csv")
df = df[df["SUMLEV"] == 50]
for state in df["STNAME"].unique():
    avg = np.average(df.where(df["STNAME"] == state).dropna() ["CENSUS2010POP"])     #'state' is a variable showing name of the state & avg of 'CENSUS2010' for all states
    print("Avg of 2010 pop of " + state + " is " + str(avg))

#OR (by using "GROUPBY" function)
for group, frame in df.groupby("STNAME"):                                            #in 'STNAME' () should be used not [], since not an index now
    avg = np.average(frame["CENSUS2010POP"])
    print("Avg of 2010 pop of " + group + " is " + str(avg))

#Function using 'groupby'
df = df.set_index("STNAME")                                                          #need to set 'STNAME' as index, else not work OR 'STNAME' is to given as groupby parameter
def batch_no(item):
    if item[0] < "M":                                                                #'0' is the index, signifying first alphabet of the 'STNAME'
        return 0
    if item[0] < "Q":
        return 1
    else:
        return 2
for group, frame in df.groupby(batch_no):                                            
    print( str(len(frame)) + " records are there in group" + str(group) + " for processing" )           #output- "1177 records are there in group0 for processing"

#Using GROUPBY on 'listings' dataset (airbnb listing of rooms)
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Administrator/Downloads/listings.csv")
df["cancellation_policy"].unique()                                                   #values present- "(['moderate', 'flexible', 'strict', 'super_strict_30'], dtype=object)"
df["review_scores_value"].unique()                                                   #values present- "([nan,  9, 10,  6,  8,  7,  2,  4,  3,  5])"
df = df.set_index( ["cancellation_policy", "review_scores_value"] )                  #setting multi-indexing for applying 'GROUPBY'
for group,frame in df.groupby(level = (0,1)):                                        #'0' for 'cancellation_policy' & '1' for 'review_scores_value'
    print(group) 

#Now separating '10' and other than '10' in 'review_scores_value'
def grouping(item):
    if item[1] == 10.0:
        return(item[0], "10")
    else:
        return(item[0], "not 10")
for group, frame in df.groupby(by = grouping):
    print(group)

#AGGREGATION(to apply single function in multiple column/groups)              
#groupby("column1").agg( {"column2" : function})

df = df.reset_index()                                                               #the index is reset
df.groupby("cancellation_policy").agg({"review_scores_value": np.nanmean})          #using mean function on 'review_scores_value' for 'cancellation_policy'

df.groupby("cancellation_policy").agg({"review_scores_value": np.nanmean,           #using mean function on 'review_scores_value' & std dev on 'reviews_per_month' for 'cancellation_policy'
                                      "reviews_per_month": np.nanstd})

#TRANSFORMATION(returns a new dataframe and returns value equal to the group i.e number of column observations)
#[["column1", "column2"]]groupby("column1").transform(function)

df1 = df[ ["cancellation_policy", "review_scores_value"] ].groupby("cancellation_policy").transform(np.nanmean)
df1.head(3)

#FILTRATION(applies a function & returns true/false based on group shold be included in result or not)
#groupby("column1").filter(function, column2, condition)

df.groupby("cancellation_policy").filter(lambda x: np.nanmean(x["review_scores_value_x"]) > 9.2)

#APPLY FUNCTION(to apply any function to each groups and stitch the results back together)
#Calculation of mean difference(mean - x)

import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Administrator/Downloads/listings.csv")                   #again the dataframe 'listing' for airbnb is imported
df = df[ ["cancellation_policy", "review_scores_value"] ]                           #new dataframe is made by using only 2 columns 
def mean_review(group):                                                             #function 'mean review' is defined
    avg = np.nanmean(group["review_scores_value"])                                  #mean of 'review_scores_value' is calculated and stored in variable 'avg'
    group["review_mean"] = np.abs(avg - group["review_scores_value"])               #new column 'review_mean' is generated ehich is equal to absolute value of (mean -x)
    return group
df.groupby("cancellation_policy").apply(mean_review).head()

#SCALES(Ratio-numeric, Interval-temprature, Ordinal-blood group, Nominal-catagorical(teams))
import pandas as pd
df = pd.DataFrame(["A+", "A", "A-", "B+", "B", "B-", "C+", "C", "D", "D-"],
                 index= ["Excellent", "Very good", "Very good", "Good", "Good", "Ok", "Poor", "Poor", "Very poor", "Very poor"],
                 columns = ["Grades"])

#To tell Python about the order of the data
category_data = pd.CategoricalDtype( categories= ["D-", "D", "C", "C+", "B-", "B", "B+", "A-", "A", "A+"],
                              ordered = True)
g = df["Grades"].astype(category_data)                                             #apply the 'category_data' variable to
g[ g > "B+" ]                                                                      #output- "A+, A, A-"

#CUT function
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Administrator/Downloads/census.csv")
df = df[ df["SUMLEV"] == 50 ]
df = df.set_index("STNAME").groupby(level = 0)["CENSUS2010POP"].agg(np.average)
pd.cut(df, 10)

#PIVOT TABLE(also a Dataframe where its row=column1, column=column2)
import pandas as pd
import numpy as np
df = pd.read_csv("C:/Users/Administrator/Downloads/university_ranking.csv")    
def category(ranking):                                                             #column name on the original dataframe is 'world_rank'
    if (ranking >= 1) & (ranking <= 100):                                          #variable name - 'ranking'
        return "First tier University"
    elif (ranking > 100) & (ranking <= 200):
        return "Second tier University"
    elif (ranking > 200) & (ranking <= 300):
        return "Third tier University"
    return "Other top University"                                                  #can not use another 'elif'
df["rank_level"] = df["world_rank"].apply(lambda x: category(x))                   #adding new column named 'rank_level'
df.pivot_table(values = "score", index = "country", columns = "rank_level",        #compare mean score from column 'score' for each 'country' based on 'rank_level'
               aggfunc = [np.mean]).head()

df.pivot_table(values = "score", index = "country", columns = "rank_level",        #2 functions simultanously- min & max 'score'
               aggfunc = [np.min, np.max]).head()

p = df.pivot_table(values = "score", index = "country", columns = "rank_level", 
               aggfunc = [np.mean], margins = True)                                #'margin' shows mean of each row in the PIVOT TABLE as well

#Use if 'idxmax' function to find the max mean
p["mean"]["First tier University"].idxmax()                                        #returns 'United Kingdom'

#Changing shape of the Pivot table(Stack & Unstack)
#Stack- lowermost column to innermost row, Unstack is just the opposite

p = p.stack()
p.head(10)

p = p.unstack().unstack()                                                          #2 times unstacked
p.head(10)     

#Date/Time functionality(Timestamp, Period & their respective index)
#Timestamp (point of time)

pd.Timestamp("2021-04-02 00:17:00")                                                #APRIL 2nd 2021
pd.Timestamp(2021, 4, 2, 0, 17)                                                    #both Time-stamp gives the same output

pd.Timestamp(2021, 4, 2, 0, 17).isoweekday()                                       #output- 5, which means its 'Friday' also Good Friday today

#Peroid (span of time)
pd.Period("2021-04")                                                               #output- "Period('2021-04', 'M')", which means Month
pd.Period("2021-04-02")                                                            #output- "Period('2021-04-02', 'D')", which means Day

pd.Period("2021-04") + 2                                                           #output- "Period('2021-06', 'M')", i.e. june 2021

#Index of Timestamp & Period
t1 = pd.Series(list("ab"), [pd.Timestamp("2021-04-02"), pd.Timestamp("2021-09-23")])          #'a' & 'b' are the index of the 2 time
type(t1.index)                                                                                #returns- 'DatetimeIndex'

t2 = pd.Series(list("cd"), [pd.Period("2021-04"), pd.Period("2021-09")])                      #'c' & 'd' are the index of the 2 Periods
type(t2.index)                                                                                #returns- 'PeriodIndex'

#Converting Datetime
import numpy as np
d1 = ["2 April 2021", "April 2,2021", "2021-04-02", "2021/04/02"]
df = pd.DataFrame(np.random.randint(10,100, (4,2)), index = d1, columns = list("AB"))         #'4' & '2' for 2 differnt columns
df.index = pd.to_datetime(df.index)                                                           #all the input dates converted into single format

#Timedelta(difference in time)
pd.Timestamp("2021-04-02 8:00AM") + pd.Timedelta("2D 3H")                                     #output- 'Timestamp('2021-04-04 11:00:00')'

#OFFSETS (e.g. - Business day)
pd.Timestamp("2021-04-02 8:00AM") + pd.offsets.Week()                                         #adds 7 days, output- 'Timestamp('2021-04-09 08:00:00')'
pd.Timestamp("2021-05-02 8:00AM") + pd.offsets.MonthEnd()                                     #shows last day of the month

#DATE-RANGE function(date, periods, freq)
pd.date_range("2021-04-02", periods = 9, freq = "2W-SUN")                         #shows 9 dates, every alternate sundays after 2nd-April
pd.date_range("2021-04-02", periods = 5, freq = "B")                              #'Business day', skips 'SAT & SUN' & shows consequtive 5 days
pd.date_range("2021-04-02", periods = 5, freq = "QS-APR")                         #'Quarter' starts from APRIL, dont write 'APRIL' instead of 'APR'

#STATISTICAL TESTING 
import pandas as pd
df = pd.read_csv("C:/Users/Administrator/Downloads/datasets/grades.csv")
print("There are {} rows & {} columns".format(df.shape[0], df.shape[1]))

#Students T-test - (ttest_ind)
#Its is to check if 2 different conditions in an experiment results in a different impact
#Actual explanation is Ha, explanation that we have is not sufficient Ho

early_finisher = df [ pd.to_datetime( df["assignment1_submission"] ) < "2016"  ]          #Students who finished their 'assignment1' by Dec 2015 will be called as 'early_finisher'
early_finisher.head()

late_finisher = df [~df.index.isin(early_finisher.index)]                                 #who are not 'early_finisher' are 'late_finisher'
late_finisher.head()

print(early_finisher ["assignment1_grade"].mean())                                        #mean of "assignment1_grade" of 'early_finisher'
print(late_finisher ["assignment1_grade"].mean())                                         #mean of "assignment1_grade" of 'late_finisher'

from scipy.stats import ttest_ind                                                         #import 'ttest_ind'
ttest_ind(early_finisher["assignment1_grade"],  late_finisher["assignment1_grade"])       #PGTARA - p-value greater than alpha reject alternate

#Creating dataframe with 100 columns and try 'ttest'
import numpy as np
df1 = pd.DataFrame( [np.random.random(100) for x in range(100)] )
df2 = pd.DataFrame( [np.random.random(100) for x in range(100)] )

def compare(alpha = 0.1):
    num_diff = 0
    for i in df1.columns:
          teststat, pval = ttest_ind(df1[i], df2[i])
          if pval <= alpha:
              num_diff = num_diff + 1
    print(num_diff)

compare()                                                                                #output- 13


